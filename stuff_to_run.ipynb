{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## create predictions file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "{(1, 2, 3, 5): 6, (2, 3, 3, 6): 8}"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = [3, 3]\n",
    "batch_sizes = [8]\n",
    "lrs = [5e-5, 1e-6, 1e-5]  # [5e-5, 1e-6]\n",
    "seeds = [42]\n",
    "\n",
    "results = {}\n",
    "results[1,2, 3, 5] = 6\n",
    "results[2, 3, 3, 6] = 8\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "results_file_path = 'Data/output/results/{model_name}_on_{dataset}_{date}'.format(\n",
    "    model_name='t5',\n",
    "    dataset='igg',\n",
    "    date='23_08_2023'\n",
    ")\n",
    "\n",
    "with open(results_file_path, 'a') as f:\n",
    "    for k, v in results.items():\n",
    "        f.write(f'ep: {k[0]}, bs: {k[1]}, lr: {k[2]}, seed: {k[3]}\\n')\n",
    "        f.write(f'accuracy = {v}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# find smallest test size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size of amazon is 8359\n",
      "test size of headlines is 5150\n",
      "test size of igg is 519\n",
      "test size of twss is 788\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/test.csv'\n",
    "test_size = None\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    print(f'test size of {dataset} is {len(df)}')\n",
    "test_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size of amazon is 2376\n",
      "label 1 count is 1188\n",
      "train size of headlines is 2376\n",
      "label 1 count is 1188\n",
      "train size of igg is 2376\n",
      "label 1 count is 1188\n",
      "train size of twss is 2376\n",
      "label 1 count is 1188\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/train.csv'\n",
    "train_size = None\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    print(f'train size of {dataset} is {len(df)}')\n",
    "    print(f'label 1 count is {len(df[df.label==1])}')\n",
    "train_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val size of amazon is 8359\n",
      "val size of headlines is 5150\n",
      "val size of igg is 519\n",
      "val size of twss is 788\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/val.csv'\n",
    "val_size = None\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    print(f'val size of {dataset} is {len(df)}')\n",
    "val_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for amazon:\n",
      "%label 1 = 50.77, %label 0 = 49.23\n",
      "for headlines:\n",
      "%label 1 = 48.86, %label 0 = 51.14\n",
      "for igg:\n",
      "%label 1 = 52.79, %label 0 = 47.21\n",
      "for twss:\n",
      "%label 1 = 47.84, %label 0 = 52.16\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/test.csv'\n",
    "max_test_size = 3500\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    df = df.iloc[:min(len(df), max_test_size)]\n",
    "    df_1 = df[df.label == 1]\n",
    "    df_0 = df[df.label == 0]\n",
    "\n",
    "    # print(f'test size of {dataset} is {len(df)}')\n",
    "    print(f'for {dataset}:')\n",
    "    print(f'%label 1 = {\"%.2f\" % (100 * len(df_1) / len(df))}, %label 0 = {\"%.2f\" % (100 * len(df_0) / len(df))}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for amazon:\n",
      "%label 1 = 49.46, %label 0 = 50.54\n",
      "for headlines:\n",
      "%label 1 = 47.11, %label 0 = 52.89\n",
      "for igg:\n",
      "%label 1 = 47.21, %label 0 = 52.79\n",
      "for twss:\n",
      "%label 1 = 49.11, %label 0 = 50.89\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/val.csv'\n",
    "max_val_size = 3500\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    df = df.iloc[:min(len(df), max_val_size)]\n",
    "    df_1 = df[df.label == 1]\n",
    "    df_0 = df[df.label == 0]\n",
    "\n",
    "    print(f'for {dataset}:')\n",
    "    print(f'%label 1 = {\"%.2f\" % (100 * len(df_1) / len(df))}, %label 0 = {\"%.2f\" % (100 * len(df_0) / len(df))}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# compute performance of the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def get_run_details(run_name):\n",
    "    run_data = run_name.split('_')\n",
    "    model = run_data[0]\n",
    "    dataset_name = run_data[2]\n",
    "    seed = run_data[3][run_data[3].index('=') + 1:]\n",
    "\n",
    "    # return model, dataset_name, float(seed)\n",
    "    return dataset_name, float(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance for ./Model/SavedModels/Bert-paired\\bert_on_amazon-headlines_seed=27\n",
      "accuracies = {'amazon': 0.5337, 'headlines': 0.5026, 'igg': 0.9075, 'twss': 0.4886}\n",
      "recall = {'amazon': 0.9842, 'headlines': 0.9813, 'igg': 0.9416, 'twss': 1.0}\n",
      "precision = {'amazon': 0.5216, 'headlines': 0.4954, 'igg': 0.8897, 'twss': 0.4833}\n",
      "performance for ./Model/SavedModels/Bert-paired\\bert_on_amazon-igg_seed=42\n",
      "accuracies = {'amazon': 0.8549, 'headlines': 0.4966, 'igg': 0.8921, 'twss': 0.4277}\n",
      "recall = {'amazon': 0.8807, 'headlines': 0.9895, 'igg': 0.9124, 'twss': 0.87}\n",
      "precision = {'amazon': 0.8409, 'headlines': 0.4924, 'igg': 0.8865, 'twss': 0.4493}\n",
      "performance for ./Model/SavedModels/Bert-paired\\bert_on_amazon-twss_seed=27\n",
      "accuracies = {'amazon': 0.8531, 'headlines': 0.53, 'igg': 0.8439, 'twss': 0.9873}\n",
      "recall = {'amazon': 0.8531, 'headlines': 0.7795, 'igg': 0.7993, 'twss': 0.9867}\n",
      "precision = {'amazon': 0.857, 'headlines': 0.5125, 'igg': 0.8939, 'twss': 0.9867}\n",
      "performance for ./Model/SavedModels/Bert-paired\\bert_on_headlines-igg_seed=18\n",
      "accuracies = {'amazon': 0.854, 'headlines': 0.5843, 'igg': 0.8979, 'twss': 0.948}\n",
      "recall = {'amazon': 0.8351, 'headlines': 0.7895, 'igg': 0.9124, 'twss': 0.9894}\n",
      "precision = {'amazon': 0.8719, 'headlines': 0.5521, 'igg': 0.8961, 'twss': 0.9098}\n",
      "performance for ./Model/SavedModels/Bert-paired\\bert_on_headlines-twss_seed=42\n",
      "accuracies = {'amazon': 0.8434, 'headlines': 0.5906, 'igg': 0.894, 'twss': 0.9873}\n",
      "recall = {'amazon': 0.8165, 'headlines': 0.7058, 'igg': 0.8759, 'twss': 0.9788}\n",
      "precision = {'amazon': 0.8673, 'headlines': 0.5648, 'igg': 0.9195, 'twss': 0.9946}\n",
      "performance for ./Model/SavedModels/Bert-paired\\bert_on_igg-twss_seed=42\n",
      "accuracies = {'amazon': 0.8277, 'headlines': 0.538, 'igg': 0.9133, 'twss': 0.9873}\n",
      "recall = {'amazon': 0.9026, 'headlines': 0.9053, 'igg': 0.938, 'twss': 0.9841}\n",
      "precision = {'amazon': 0.7886, 'headlines': 0.5155, 'igg': 0.9018, 'twss': 0.9893}\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "import glob\n",
    "import os\n",
    "\n",
    "output_path = './Data/output/results/'\n",
    "# dataset_names = ['amazon', 'headlines', 'igg', 'twss']\n",
    "dataset_names = [\"amazon-headlines\", \"amazon-igg\", \"amazon-twss\", \"headlines-igg\", \"headlines-twss\", \"igg-twss\"]\n",
    "data_path = './Data/humor_datasets/'\n",
    "split_type = 'with_val_fixed_train'\n",
    "models_path = './Model/SavedModels/Bert-paired'\n",
    "base_model = 'bert'\n",
    "models_name = [glob.glob(f'{models_path}/{base_model}_on_{dataset}*')[0] for dataset in dataset_names]\n",
    "\n",
    "df = pd.read_excel(output_path + 'humor_results_template.xlsx')\n",
    "df.fillna(method='ffill', axis=0, inplace=True)\n",
    "df.set_index(['performance', 'model', 'trained on', 'seed'], inplace=True)\n",
    "\n",
    "for model_name in models_name:\n",
    "    # base_model, dataset_name, seed = get_run_details(model_name)\n",
    "    dataset_name, seed = get_run_details(model_name)\n",
    "    pred_path = model_name + '/predictions/'\n",
    "    accuracies = {}\n",
    "    recall = {}\n",
    "    precision = {}\n",
    "    predict_dataset_names = ['amazon', 'headlines', 'igg', 'twss']\n",
    "    for dataset in predict_dataset_names:\n",
    "        pred_labels_path = pred_path + f'{dataset}_preds.csv'\n",
    "        test_labels_path = data_path + f'{dataset}/{split_type}/test.csv'\n",
    "        if not (exists(pred_labels_path) and exists(test_labels_path)):\n",
    "            print('didnt find preds/test path')\n",
    "            continue\n",
    "\n",
    "        _preds = pd.read_csv(pred_labels_path)\n",
    "        _test = pd.read_csv(test_labels_path)\n",
    "        _test = _test.iloc[:len(_preds)]\n",
    "        if (len(_preds[_preds.label == -1]) > 0):\n",
    "            illegal_indices = _preds[_preds.label == -1].index\n",
    "            print(f'there are {len(illegal_indices)} illegal indices in {dataset_name} predictions on {dataset}')\n",
    "            _preds = _preds.drop(labels=illegal_indices, axis=0)\n",
    "            _test = _test.drop(labels=illegal_indices, axis=0)\n",
    "        accuracies[dataset] = float(\"%.4f\" % accuracy_score(_test.label, _preds.label))\n",
    "        recall[dataset] = float(\"%.4f\" % recall_score(_test.label, _preds.label))\n",
    "        precision[dataset] = float(\"%.4f\" % precision_score(_test.label, _preds.label))\n",
    "\n",
    "    print(f'performance for {model_name}')\n",
    "    print(f'accuracies = {accuracies}')\n",
    "    print(f'recall = {recall}')\n",
    "    print(f'precision = {precision}')\n",
    "\n",
    "    df.loc[('accuracy', base_model, dataset_name, seed)] = accuracies\n",
    "    df.loc[('recall', base_model, dataset_name, seed)] = recall\n",
    "    df.loc[('precision', base_model, dataset_name, seed)] = precision\n",
    "\n",
    "# save performance to output file\n",
    "i = 0\n",
    "while os.path.exists(output_path + f'humor_results_{i}.xlsx'):\n",
    "    i += 1\n",
    "\n",
    "df.to_excel(output_path + f'humor_results_{i}.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# compute T5 models mean & std accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon: 0.8551 +- 0.001\n",
      "headlines: 0.5816 +- 0.001\n",
      "igg: 0.9370 +- 0.002\n",
      "twss: 0.5555 +- 0.141\n"
     ]
    }
   ],
   "source": [
    "acc_igg = [0.9347826086956522, 0.9391304347826087, 0.9376811594202898, 0.936231884057971]\n",
    "acc_amazon = [0.8557142857142858, 0.8542857142857143, 0.8551428571428571, 0.8554285714285714]\n",
    "acc_headlines = [0.5831428571428572, 0.5805714285714285, 0.5822857142857143, 0.5805714285714285]\n",
    "acc_twss = [0.45634920634920634, 0.4777636594663278, 0.4885786802030457, 0.799492385786802]\n",
    "accs = {'amazon': acc_amazon, 'headlines': acc_headlines, 'igg': acc_igg, 'twss': acc_twss}\n",
    "for k,v in accs.items():\n",
    "    print(f'{k}: {\"%.4f\" % np.mean(v)} +- {\"%.3f\" % np.std(v)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# compute Bert models mean & std accuracy\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon: 0.8361 +- 0.004\n",
      "headlines: 0.5954 +- 0.011\n",
      "igg: 0.8998 +- 0.015\n",
      "twss: 0.9940 +- 0.002\n"
     ]
    }
   ],
   "source": [
    "acc_igg = [0.9094412331406551, 0.9113680154142582, 0.9036608863198459, 0.8747591522157996]\n",
    "acc_amazon = [0.8414285714285714, 0.8368571428571429, 0.8305714285714285, 0.8354285714285714]\n",
    "acc_headlines = [0.5834285714285714, 0.586, 0.6031428571428571, 0.6091428571428571]\n",
    "acc_twss = [0.9949238578680203, 0.9898477157360406, 0.9961928934010152, 0.9949238578680203]\n",
    "accs = {'amazon': acc_amazon, 'headlines': acc_headlines, 'igg': acc_igg, 'twss': acc_twss}\n",
    "for k,v in accs.items():\n",
    "    print(f'{k}: {\"%.4f\" % np.mean(v)} +- {\"%.3f\" % np.std(v)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# compute Bert models on pairs mean & std"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "amazon_headlines = [0.9094412331406551, 0.9113680154142582, 0.9036608863198459, 0.8747591522157996]\n",
    "acc_amazon = [0.8414285714285714, 0.8368571428571429, 0.8305714285714285, 0.8354285714285714]\n",
    "acc_headlines = [0.5834285714285714, 0.586, 0.6031428571428571, 0.6091428571428571]\n",
    "acc_twss = [0.9949238578680203, 0.9898477157360406, 0.9961928934010152, 0.9949238578680203]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ep: 3, bs: 8, lr: 5e-05, seed: 5\n",
    "accuracy = 0.9949238578680203, 0.9898477157360406, 0.9961928934010152, 0.9949238578680203\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}