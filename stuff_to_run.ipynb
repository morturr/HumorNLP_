{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Trying Kfold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_path = './Data/new_humor_datasets/amazon/data.csv'\n",
    "df = pd.read_csv(data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "# result = next(kf.split(df), None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([    0,     1,     2, ..., 19091, 19092, 19093]), array([    4,     5,     6, ..., 19085, 19088, 19089]))\n",
      "(array([    1,     2,     3, ..., 19090, 19092, 19093]), array([    0,    12,    17, ..., 19078, 19087, 19091]))\n",
      "(array([    0,     1,     3, ..., 19091, 19092, 19093]), array([    2,     7,    10, ..., 19054, 19063, 19064]))\n",
      "(array([    0,     2,     4, ..., 19091, 19092, 19093]), array([    1,     3,     9, ..., 19084, 19086, 19090]))\n",
      "(array([    0,     1,     2, ..., 19089, 19090, 19091]), array([    8,    14,    15, ..., 19081, 19092, 19093]))\n"
     ]
    }
   ],
   "source": [
    "for split in kf.split(df):\n",
    "    print(split)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "splits = list(kf.split(df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "dd = DatasetDict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dd['train'] = train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train = df.iloc[result[0]]\n",
    "test =  df.iloc[result[1]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "result = next(kf.split(df), None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trying LLama 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2432ee2310e34a749e077b7b36421eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mortu\\PycharmProjects\\HumorNLP_\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mortu\\.cache\\huggingface\\hub\\models--meta-llama--Llama-2-7b-chat-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffcd15507e84fc7bf74947fc88bdc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84797e04efe6471abc5c4c29780239be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e3af8611ed4bb1b932b04129acbc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f617dbf8af743b2947f3b7c13551cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2309884c2be34d008901425c611da7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta-llama/Llama-2-7b-chat-hf\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      7\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create predictions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2, 3, 5): 6, (2, 3, 3, 6): 8}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = [3, 3]\n",
    "batch_sizes = [8]\n",
    "lrs = [5e-5, 1e-6, 1e-5]  # [5e-5, 1e-6]\n",
    "seeds = [42]\n",
    "\n",
    "results = {}\n",
    "results[1,2, 3, 5] = 6\n",
    "results[2, 3, 3, 6] = 8\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_file_path = 'Data/output/results/{model_name}_on_{dataset}_{date}'.format(\n",
    "    model_name='t5',\n",
    "    dataset='igg',\n",
    "    date='23_08_2023'\n",
    ")\n",
    "\n",
    "with open(results_file_path, 'a') as f:\n",
    "    for k, v in results.items():\n",
    "        f.write(f'ep: {k[0]}, bs: {k[1]}, lr: {k[2]}, seed: {k[3]}\\n')\n",
    "        f.write(f'accuracy = {v}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# find smallest test size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size of amazon is 8359\n",
      "test size of headlines is 5150\n",
      "test size of igg is 519\n",
      "test size of twss is 788\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/test.csv'\n",
    "test_size = None\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    print(f'test size of {dataset} is {len(df)}')\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size of amazon is 2376\n",
      "label 1 count is 1188\n",
      "train size of headlines is 2376\n",
      "label 1 count is 1188\n",
      "train size of igg is 2376\n",
      "label 1 count is 1188\n",
      "train size of twss is 2376\n",
      "label 1 count is 1188\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/train.csv'\n",
    "train_size = None\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    print(f'train size of {dataset} is {len(df)}')\n",
    "    print(f'label 1 count is {len(df[df.label==1])}')\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val size of amazon is 8359\n",
      "val size of headlines is 5150\n",
      "val size of igg is 519\n",
      "val size of twss is 788\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/val.csv'\n",
    "val_size = None\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    print(f'val size of {dataset} is {len(df)}')\n",
    "val_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# distribution of test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for amazon:\n",
      "%label 1 = 50.77, %label 0 = 49.23\n",
      "for headlines:\n",
      "%label 1 = 48.86, %label 0 = 51.14\n",
      "for igg:\n",
      "%label 1 = 52.79, %label 0 = 47.21\n",
      "for twss:\n",
      "%label 1 = 47.84, %label 0 = 52.16\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/test.csv'\n",
    "max_test_size = 3500\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    df = df.iloc[:min(len(df), max_test_size)]\n",
    "    df_1 = df[df.label == 1]\n",
    "    df_0 = df[df.label == 0]\n",
    "\n",
    "    # print(f'test size of {dataset} is {len(df)}')\n",
    "    print(f'for {dataset}:')\n",
    "    print(f'%label 1 = {\"%.2f\" % (100 * len(df_1) / len(df))}, %label 0 = {\"%.2f\" % (100 * len(df_0) / len(df))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## distribution of val labels in tripled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for amazon_headlines_igg:\n",
      "val size = 1557\n",
      "%label 1 = 48.30, %label 0 = 51.70\n",
      "for amazon_headlines_twss:\n",
      "val size = 1557\n",
      "%label 1 = 49.20, %label 0 = 50.80\n",
      "for amazon_igg_twss:\n",
      "val size = 1557\n",
      "%label 1 = 49.00, %label 0 = 51.00\n",
      "for headlines_igg_twss:\n",
      "val size = 1557\n",
      "%label 1 = 48.30, %label 0 = 51.70\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/tripled_datasets/{dataset}/with_val_fixed_train/val.csv'\n",
    "max_test_size = 3500\n",
    "datasets = ['amazon_headlines_igg', 'amazon_headlines_twss', 'amazon_igg_twss', 'headlines_igg_twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    df = df.iloc[:min(len(df), max_test_size)]\n",
    "    df_1 = df[df.label == 1]\n",
    "    df_0 = df[df.label == 0]\n",
    "\n",
    "    # print(f'test size of {dataset} is {len(df)}')\n",
    "    print(f'for {dataset}:')\n",
    "    print(f'val size = {len(df)}')\n",
    "    print(f'%label 1 = {\"%.2f\" % (100 * len(df_1) / len(df))}, %label 0 = {\"%.2f\" % (100 * len(df_0) / len(df))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# distribution of paired val label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for amazon_headlines:\n",
      "val size = 1038\n",
      "%label 1 = 48.84, %label 0 = 51.16\n",
      "for amazon_igg:\n",
      "val size = 1038\n",
      "%label 1 = 48.55, %label 0 = 51.45\n",
      "for amazon_twss:\n",
      "val size = 1038\n",
      "%label 1 = 49.90, %label 0 = 50.10\n",
      "for headlines_igg:\n",
      "val size = 1038\n",
      "%label 1 = 47.50, %label 0 = 52.50\n",
      "for headlines_twss:\n",
      "val size = 1038\n",
      "%label 1 = 48.84, %label 0 = 51.16\n",
      "for igg_twss:\n",
      "val size = 1038\n",
      "%label 1 = 48.55, %label 0 = 51.45\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/paired_datasets/{dataset}/with_val_fixed_train/val.csv'\n",
    "max_val_size = 3500\n",
    "datasets = [\"amazon_headlines\", \"amazon_igg\", \"amazon_twss\", \"headlines_igg\", \"headlines_twss\", \"igg_twss\"]\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    df = df.iloc[:min(len(df), max_val_size)]\n",
    "    df_1 = df[df.label == 1]\n",
    "    df_0 = df[df.label == 0]\n",
    "\n",
    "    print(f'for {dataset}:')\n",
    "    print(f'val size = {len(df)}')\n",
    "    print(f'%label 1 = {\"%.2f\" % (100 * len(df_1) / len(df))}, %label 0 = {\"%.2f\" % (100 * len(df_0) / len(df))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# distribution of val labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/humor_datasets/amazon_headlines/with_val_fixed_train/val.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_19700/1306269948.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mdatasets\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m\"amazon_headlines\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"amazon_igg\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"amazon_twss\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"headlines_igg\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"headlines_twss\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"igg_twss\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mdataset\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m     \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mmin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_val_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[0mdf_1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlabel\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m                 )\n\u001B[1;32m--> 311\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 586\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    587\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    588\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    480\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    481\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 482\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    483\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    484\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    810\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 811\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    812\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    813\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1038\u001B[0m             )\n\u001B[0;32m   1039\u001B[0m         \u001B[1;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1040\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1041\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1042\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m         \u001B[1;31m# open handles\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 51\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     52\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001B[0m in \u001B[0;36m_open_handles\u001B[1;34m(self, src, kwds)\u001B[0m\n\u001B[0;32m    220\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    221\u001B[0m         \"\"\"\n\u001B[1;32m--> 222\u001B[1;33m         self.handles = get_handle(\n\u001B[0m\u001B[0;32m    223\u001B[0m             \u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    224\u001B[0m             \u001B[1;34m\"r\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001B[0m in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    700\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;34m\"b\"\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    701\u001B[0m             \u001B[1;31m# Encoding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 702\u001B[1;33m             handle = open(\n\u001B[0m\u001B[0;32m    703\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    704\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './Data/humor_datasets/amazon_headlines/with_val_fixed_train/val.csv'"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/val.csv'\n",
    "max_val_size = 3500\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    df = df.iloc[:min(len(df), max_val_size)]\n",
    "    df_1 = df[df.label == 1]\n",
    "    df_0 = df[df.label == 0]\n",
    "\n",
    "    print(f'for {dataset}:')\n",
    "    print(f'%label 1 = {\"%.2f\" % (100 * len(df_1) / len(df))}, %label 0 = {\"%.2f\" % (100 * len(df_0) / len(df))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# compute performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_run_details(run_name):\n",
    "    run_data = run_name.split('_')\n",
    "    model = run_data[0]\n",
    "    dataset_name = run_data[2]\n",
    "    seed = run_data[3][run_data[3].index('=') + 1:]\n",
    "\n",
    "    # return model, dataset_name, float(seed)\n",
    "    return dataset_name, float(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# compute T5 models mean & std accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon: 0.8551 +- 0.001\n",
      "headlines: 0.5816 +- 0.001\n",
      "igg: 0.9370 +- 0.002\n",
      "twss: 0.5555 +- 0.141\n"
     ]
    }
   ],
   "source": [
    "acc_igg = [0.9347826086956522, 0.9391304347826087, 0.9376811594202898, 0.936231884057971]\n",
    "acc_amazon = [0.8557142857142858, 0.8542857142857143, 0.8551428571428571, 0.8554285714285714]\n",
    "acc_headlines = [0.5831428571428572, 0.5805714285714285, 0.5822857142857143, 0.5805714285714285]\n",
    "acc_twss = [0.45634920634920634, 0.4777636594663278, 0.4885786802030457, 0.799492385786802]\n",
    "accs = {'amazon': acc_amazon, 'headlines': acc_headlines, 'igg': acc_igg, 'twss': acc_twss}\n",
    "for k,v in accs.items():\n",
    "    print(f'{k}: {\"%.4f\" % np.mean(v)} +- {\"%.3f\" % np.std(v)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# compute Bert models mean & std accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon: 0.8361 +- 0.004\n",
      "headlines: 0.5954 +- 0.011\n",
      "igg: 0.8998 +- 0.015\n",
      "twss: 0.9940 +- 0.002\n"
     ]
    }
   ],
   "source": [
    "acc_igg = [0.9094412331406551, 0.9113680154142582, 0.9036608863198459, 0.8747591522157996]\n",
    "acc_amazon = [0.8414285714285714, 0.8368571428571429, 0.8305714285714285, 0.8354285714285714]\n",
    "acc_headlines = [0.5834285714285714, 0.586, 0.6031428571428571, 0.6091428571428571]\n",
    "acc_twss = [0.9949238578680203, 0.9898477157360406, 0.9961928934010152, 0.9949238578680203]\n",
    "accs = {'amazon': acc_amazon, 'headlines': acc_headlines, 'igg': acc_igg, 'twss': acc_twss}\n",
    "for k,v in accs.items():\n",
    "    print(f'{k}: {\"%.4f\" % np.mean(v)} +- {\"%.3f\" % np.std(v)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/output/results/humor_results_template.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 15\u001B[0m\n\u001B[0;32m     12\u001B[0m base_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     13\u001B[0m models_name \u001B[38;5;241m=\u001B[39m [glob\u001B[38;5;241m.\u001B[39mglob(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodels_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbase_model\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_on_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m dataset_names]\n\u001B[1;32m---> 15\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_excel\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhumor_results_template.xlsx\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m df\u001B[38;5;241m.\u001B[39mfillna(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mffill\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     17\u001B[0m df\u001B[38;5;241m.\u001B[39mset_index([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mperformance\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrained on\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m'\u001B[39m], inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\HumorNLP_\\venv\\lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001B[0m, in \u001B[0;36mread_excel\u001B[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001B[0m\n\u001B[0;32m    493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(io, ExcelFile):\n\u001B[0;32m    494\u001B[0m     should_close \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 495\u001B[0m     io \u001B[38;5;241m=\u001B[39m \u001B[43mExcelFile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m engine \u001B[38;5;129;01mand\u001B[39;00m engine \u001B[38;5;241m!=\u001B[39m io\u001B[38;5;241m.\u001B[39mengine:\n\u001B[0;32m    502\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    503\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEngine should not be specified when passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    504\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man ExcelFile - ExcelFile already has the engine set\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    505\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\HumorNLP_\\venv\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001B[0m, in \u001B[0;36mExcelFile.__init__\u001B[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001B[0m\n\u001B[0;32m   1548\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxls\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1549\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1550\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[43minspect_excel_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1551\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\n\u001B[0;32m   1552\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1553\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ext \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1554\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1555\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExcel file format cannot be determined, you must specify \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1556\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man engine manually.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1557\u001B[0m         )\n",
      "File \u001B[1;32m~\\PycharmProjects\\HumorNLP_\\venv\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001B[0m, in \u001B[0;36minspect_excel_format\u001B[1;34m(content_or_path, storage_options)\u001B[0m\n\u001B[0;32m   1399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(content_or_path, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[0;32m   1400\u001B[0m     content_or_path \u001B[38;5;241m=\u001B[39m BytesIO(content_or_path)\n\u001B[1;32m-> 1402\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1403\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m   1404\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handle:\n\u001B[0;32m   1405\u001B[0m     stream \u001B[38;5;241m=\u001B[39m handle\u001B[38;5;241m.\u001B[39mhandle\n\u001B[0;32m   1406\u001B[0m     stream\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\HumorNLP_\\venv\\lib\\site-packages\\pandas\\io\\common.py:882\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    874\u001B[0m             handle,\n\u001B[0;32m    875\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    878\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    879\u001B[0m         )\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m--> 882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    883\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[0;32m    885\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './Data/output/results/humor_results_template.xlsx'"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "import glob\n",
    "import os\n",
    "\n",
    "output_path = './Data/output/results/'\n",
    "# dataset_names = ['amazon', 'headlines', 'igg', 'twss']\n",
    "dataset_names = [\"amazon-headlines\", \"amazon-igg\", \"amazon-twss\", \"headlines-igg\", \"headlines-twss\", \"igg-twss\"]\n",
    "data_path = './Data/humor_datasets/'\n",
    "split_type = 'with_val_fixed_train'\n",
    "models_path = './Model/SavedModels/Bert-paired'\n",
    "base_model = 'bert'\n",
    "models_name = [glob.glob(f'{models_path}/{base_model}_on_{dataset}*')[0] for dataset in dataset_names]\n",
    "\n",
    "df = pd.read_excel(output_path + 'humor_results_template.xlsx')\n",
    "df.fillna(method='ffill', axis=0, inplace=True)\n",
    "df.set_index(['performance', 'model', 'trained on', 'seed'], inplace=True)\n",
    "\n",
    "for model_name in models_name:\n",
    "    # base_model, dataset_name, seed = get_run_details(model_name)\n",
    "    dataset_name, seed = get_run_details(model_name)\n",
    "    pred_path = model_name + '/predictions/'\n",
    "    accuracies = {}\n",
    "    recall = {}\n",
    "    precision = {}\n",
    "    predict_dataset_names = ['amazon', 'headlines', 'igg', 'twss']\n",
    "    for dataset in predict_dataset_names:\n",
    "        pred_labels_path = pred_path + f'{dataset}_preds.csv'\n",
    "        test_labels_path = data_path + f'{dataset}/{split_type}/test.csv'\n",
    "        if not (exists(pred_labels_path) and exists(test_labels_path)):\n",
    "            print('didnt find preds/test path')\n",
    "            continue\n",
    "\n",
    "        _preds = pd.read_csv(pred_labels_path)\n",
    "        _test = pd.read_csv(test_labels_path)\n",
    "        _test = _test.iloc[:len(_preds)]\n",
    "        if (len(_preds[_preds.label == -1]) > 0):\n",
    "            illegal_indices = _preds[_preds.label == -1].index\n",
    "            print(f'there are {len(illegal_indices)} illegal indices in {dataset_name} predictions on {dataset}')\n",
    "            _preds = _preds.drop(labels=illegal_indices, axis=0)\n",
    "            _test = _test.drop(labels=illegal_indices, axis=0)\n",
    "        accuracies[dataset] = float(\"%.4f\" % accuracy_score(_test.label, _preds.label))\n",
    "        recall[dataset] = float(\"%.4f\" % recall_score(_test.label, _preds.label))\n",
    "        precision[dataset] = float(\"%.4f\" % precision_score(_test.label, _preds.label))\n",
    "\n",
    "    print(f'performance for {model_name}')\n",
    "    print(f'accuracies = {accuracies}')\n",
    "    print(f'recall = {recall}')\n",
    "    print(f'precision = {precision}')\n",
    "\n",
    "    df.loc[('accuracy', base_model, dataset_name, seed)] = accuracies\n",
    "    df.loc[('recall', base_model, dataset_name, seed)] = recall\n",
    "    df.loc[('precision', base_model, dataset_name, seed)] = precision\n",
    "\n",
    "# save performance to output file\n",
    "i = 0\n",
    "while os.path.exists(output_path + f'humor_results_{i}.xlsx'):\n",
    "    i += 1\n",
    "\n",
    "df.to_excel(output_path + f'humor_results_{i}.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# compute T5 models on pairs mean & std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_healines: 0.8299 +- 0.038\n",
      "amazon_igg: 0.8386 +- 0.019\n",
      "amazon_twss: 0.8500 +- 0.004\n",
      "headlines_igg: 0.5812 +- 0.002\n",
      "headlines_twss: 0.9841 +- 0.002\n",
      "igg_twss: 0.9075 +- 0.003\n"
     ]
    }
   ],
   "source": [
    "amazon_headlines_on_amazon = [0.7637142857142857, 0.8462857142857143, 0.8562857142857143, 0.8534285714285714]\n",
    "amazon_igg_on_amazon = [0.8071428571428572, 0.842, 0.8505714285714285, 0.8548571428571429]\n",
    "amazon_twss_on_amazon = [0.8431428571428572, 0.8505714285714285, 0.8531428571428571, 0.8531428571428571]\n",
    "headlines_igg_on_headlines = [0.5791428571428572, 0.5842857142857143, 0.5828571428571429, 0.5785714285714286]\n",
    "headlines_twss_on_twss = [0.9822335025380711, 0.983502538071066, 0.983502538071066, 0.9873096446700508]\n",
    "igg_twss_on_igg = [0.905587668593449, 0.905587668593449, 0.905587668593449, 0.9132947976878613]\n",
    "accs = {'amazon_healines': amazon_headlines_on_amazon,\n",
    "        'amazon_igg': amazon_igg_on_amazon,\n",
    "        'amazon_twss': amazon_twss_on_amazon,\n",
    "        'headlines_igg': headlines_igg_on_headlines,\n",
    "        'headlines_twss': headlines_twss_on_twss,\n",
    "        'igg_twss': igg_twss_on_igg}\n",
    "\n",
    "for k,v in accs.items():\n",
    "    print(f'{k}: {\"%.4f\" % np.mean(v)} +- {\"%.3f\" % np.std(v)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# compute Bert models on pairs mean & std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_healines: 0.8295 +- 0.003\n",
      "amazon_igg: 0.8259 +- 0.006\n",
      "amazon_twss: 0.8068 +- 0.006\n",
      "headlines_igg: 0.8786 +- 0.005\n",
      "headlines_twss: 0.9854 +- 0.003\n",
      "igg_twss: 0.8420 +- 0.026\n"
     ]
    }
   ],
   "source": [
    "amazon_headlines_on_amazon = [0.8328571428571429, 0.83, 0.8294285714285714, 0.8257142857142857]\n",
    "amazon_igg_on_amazon = [0.8345714285714285, 0.8274285714285714, 0.8214285714285714, 0.8202857142857143]\n",
    "amazon_twss_on_amazon = [0.8105714285714286, 0.812, 0.7977142857142857, 0.8068571428571428]\n",
    "headlines_igg_on_igg = [0.8728323699421965, 0.8766859344894027, 0.8786127167630058, 0.8863198458574181]\n",
    "headlines_twss_on_twss = [0.9796954314720813, 0.9873096446700508, 0.9885786802030457, 0.9860406091370558]\n",
    "igg_twss_on_igg = [0.861271676300578, 0.8535645472061657, 0.7976878612716763, 0.8554913294797688]\n",
    "accs = {'amazon_healines': amazon_headlines_on_amazon,\n",
    "        'amazon_igg': amazon_igg_on_amazon,\n",
    "        'amazon_twss': amazon_twss_on_amazon,\n",
    "        'headlines_igg': headlines_igg_on_igg,\n",
    "        'headlines_twss': headlines_twss_on_twss,\n",
    "        'igg_twss': igg_twss_on_igg}\n",
    "\n",
    "for k,v in accs.items():\n",
    "    print(f'{k}: {\"%.4f\" % np.mean(v)} +- {\"%.3f\" % np.std(v)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# check headlines dataset meanGrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_headlines_path = './Data/humor_datasets/headlines/with_val_fixed_train/{split}.csv'\n",
    "processed_train_df = pd.read_csv(processed_headlines_path.format(split='train'))\n",
    "processed_test_df = pd.read_csv(processed_headlines_path.format(split='test'))\n",
    "processed_val_df = pd.read_csv(processed_headlines_path.format(split='val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_headlines_path = './Data/original_datasets/headlines/{split}.csv'\n",
    "original_train_df = pd.read_csv(original_headlines_path.format(split='train'))\n",
    "original_test_df = pd.read_csv(original_headlines_path.format(split='test'))\n",
    "original_all = original_train_df.append(original_test_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_column_data(column_name):\n",
    "    def add_col_to_row(row):\n",
    "        origin_row = original_all[original_all['id'] == row['id']].squeeze()\n",
    "        return origin_row[column_name]\n",
    "    return add_col_to_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['meanGrade', 'original', 'edit']\n",
    "for col in cols:\n",
    "    processed_train_df[col] = processed_train_df.apply(add_column_data(col), axis=1)\n",
    "    processed_test_df[col] = processed_test_df.apply(add_column_data(col), axis=1)\n",
    "    processed_val_df[col] = processed_val_df.apply(add_column_data(col), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_val_size = 3500\n",
    "processed_test_df = processed_test_df.iloc[:max_val_size]\n",
    "processed_val_df = processed_val_df.iloc[:max_val_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Grade stats:\n",
      "-- train --\n",
      "mean = 0.9420454545454544, std = 0.5772500621369475\n",
      "-- test --\n",
      "mean = 0.9382, std = 0.5886325917698874\n",
      "-- val --\n",
      "mean = 0.9175238095238093, std = 0.5766939173305733\n"
     ]
    }
   ],
   "source": [
    "print('Mean Grade stats:')\n",
    "print(f'-- train --')\n",
    "print(f'mean = {processed_train_df.meanGrade.mean()}, std = {processed_train_df.meanGrade.std()}')\n",
    "print(f'-- test --')\n",
    "print(f'mean = {processed_test_df.meanGrade.mean()}, std = {processed_test_df.meanGrade.std()}')\n",
    "print(f'-- val --')\n",
    "print(f'mean = {processed_val_df.meanGrade.mean()}, std = {processed_val_df.meanGrade.std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "sample_train = processed_train_df.iloc[:sample_size]\n",
    "sample_test = processed_test_df.iloc[:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_path = './Data/output/headlines_data_samples/'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "sample_train.to_csv(output_path + 'train.csv', index=False)\n",
    "sample_test.to_csv(output_path + 'test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check headlines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "headlines_path = './Data/humor_datasets/headlines/{sanity}with_val_fixed_train'\n",
    "real_train = pd.read_csv(headlines_path.format(sanity='') + '/train.csv')\n",
    "real_test = pd.read_csv(headlines_path.format(sanity='') + '/test.csv')\n",
    "sanity_train = pd.read_csv(headlines_path.format(sanity='sanity-check/') + '/train.csv')\n",
    "sanity_test = pd.read_csv(headlines_path.format(sanity='sanity-check/') + '/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_headlines_path = './Data/original_datasets/headlines/{split}.csv'\n",
    "original_train = pd.read_csv(original_headlines_path.format(split='train'))\n",
    "original_test = pd.read_csv(original_headlines_path.format(split='test'))\n",
    "all_data = original_test.append(original_train, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def edit_headline(row):\n",
    "    headline = row['original']\n",
    "    edit_word = row['edit']\n",
    "    res = headline[:headline.index('<')] + edit_word + headline[headline.index('>') + 1:]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_data['edited_sentence'] = all_data.apply(edit_headline, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_mean_grade(row):\n",
    "    origin_row = all_data[all_data['id'] == row['id']].squeeze()\n",
    "    return origin_row['meanGrade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "real_train['meanGrade'] = real_train.apply(add_mean_grade, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# T5 on all headlines acccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines accuracy: mean = 0.6258, std = 0.0063\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "headlines_accuracies = [0.6127912741695587, 0.6326227069905801, 0.6351016360932077, 0.6271690629647992,\n",
    "0.6256817055032227, 0.6256817055032227, 0.624194348041646, 0.623202776400595]\n",
    "print(f'headlines accuracy: mean = {\"%.4f\" % np.mean(headlines_accuracies)}, std = {\"%.4f\" % np.std(headlines_accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Trying kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_path = './Data/humor_datasets/headlines/kfold_cv/'\n",
    "data = pd.read_csv(data_path + 'data.csv')\n",
    "os.makedirs(data_path + 'folds/', exist_ok=True)\n",
    "kfold = StratifiedKFold(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size = 9295, test size = 3099\n",
      "label 1 % = 49.994620763851536\n",
      "train size = 9295, test size = 3099\n",
      "label 1 % = 50.005379236148464\n",
      "train size = 9296, test size = 3098\n",
      "label 1 % = 50.0\n",
      "train size = 9296, test size = 3098\n",
      "label 1 % = 50.0\n"
     ]
    }
   ],
   "source": [
    "for i, indices in enumerate(kfold.split(data, data['label'])):\n",
    "    train = indices[0]\n",
    "    test = indices[1]\n",
    "    # print('train: %s, test: %s' % (train, test))\n",
    "    print(f'train size = {len(train)}, test size = {len(test)}')\n",
    "    data_train = data.iloc[train]\n",
    "    print(f'label 1 % = {100 * len(data_train[data_train.label == 1]) / len(data_train)}')\n",
    "    data_test = data.iloc[test]\n",
    "    data_test, data_val = train_test_split(data_test, test_size=0.5, shuffle=True, random_state=0)\n",
    "    output_path = data_path + f'folds/fold_{i}/with_val/'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    data_train.to_csv(output_path + 'train.csv', index=False)\n",
    "    data_test.to_csv(output_path + 'test.csv', index=False)\n",
    "    data_val.to_csv(output_path + 'val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size = 2560, test size = 854\n",
      "label 1 % = 49.609375\n",
      "train size = 2560, test size = 854\n",
      "label 1 % = 50.1953125\n",
      "train size = 2561, test size = 853\n",
      "label 1 % = 51.34713002733307\n",
      "train size = 2561, test size = 853\n",
      "label 1 % = 50.488090589613435\n"
     ]
    }
   ],
   "source": [
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "output_path = './Data/humor_datasets/{dataset}/kfold_cv/'\n",
    "input_path = './Data/humor_datasets/{dataset}/data.csv'\n",
    "kfold = StratifiedKFold(n_splits=4)\n",
    "\n",
    "# compute fixed train size by igg train size\n",
    "igg_df = pd.read_csv(output_path.format(dataset='igg') + 'balanced_data.csv')\n",
    "splits = kfold.split(igg_df, igg_df['label'])\n",
    "for train, test in splits:\n",
    "    # print('train: %s, test: %s' % (train, test))\n",
    "    print(f'train size = {len(train)}, test size = {len(test)}')\n",
    "    data_train = data.iloc[train]\n",
    "    print(f'label 1 % = {100 * len(data_train[data_train.label == 1]) / len(data_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are common rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "1  2  Y\n",
       "2  3  Z"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({'A': [1, 2, 3, 4],\n",
    "                    'B': ['X', 'Y', 'Z', 'W']})\n",
    "\n",
    "df2 = pd.DataFrame({'A': [3, 2, 5, 6],\n",
    "                    'B': ['Z', 'Y', 'P', 'Q']})\n",
    "\n",
    "# Check for common rows\n",
    "# common_rows = df1[df1.isin(df2.to_dict(orient='list')).all(axis=1)]\n",
    "common_rows = df1[df1['A'].isin(df2['A'])]\n",
    "\n",
    "# Check if there are common rows\n",
    "if not common_rows.empty:\n",
    "    print(\"There are common rows.\")\n",
    "else:\n",
    "    print(\"There are no common rows.\")\n",
    "\n",
    "common_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        A       B  C\n",
      "0  value1  value2  1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {'A': ['value1', 'value2', 'value3', 'value4'],\n",
    "        'B': ['value2', 'value2', 'value1', 'value3'],\n",
    "        'C': [1, 2, 3, 4]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the conditions for each column\n",
    "condition1 = df['A'] == 'value1'\n",
    "condition2 = df['B'] == 'value2'\n",
    "\n",
    "# Combine the conditions using the AND operator (&)\n",
    "result = df[condition1 & condition2]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./Data/data_analysis/sentence_length_summarized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./files_from_cluster/models_performance_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pairs_df_1 = pd.read_csv('./files_from_cluster/pair_models_1.csv')\n",
    "pairs_df_2 = pd.read_csv('./files_from_cluster/pair_models_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pairs_df = pairs_df_1.append(pairs_df_2, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pairs_df.to_csv('./files_from_cluster/pair_models.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## compute mean and std of the performance of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df = pd.read_csv('./files_from_cluster/pair_models.csv')\n",
    "\n",
    "single_datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "paired_datasets = ['amazon_headlines', 'amazon_igg', 'amazon_twss', 'headlines_igg', 'headlines_twss', 'igg_twss']\n",
    "cols = ['model','trained_on', 'seed', 'predict_on' ,'mean_accuracy', 'std_accuracy', 'accuracy', 'recall', 'precision']\n",
    "cols2 = ['performance', 'model','trained_on', 'seed', 'amazon', 'headlines', 'igg', 'twss']\n",
    "\n",
    "datasets = [('single', single_datasets), ('pair', paired_datasets)]\n",
    "datasets = [('pair', paired_datasets)]\n",
    "# datasets = [('single', single_datasets)]\n",
    "\n",
    "output_path = './Data/output/results/'\n",
    "\n",
    "results_df = pd.read_excel(output_path + 'humor_results_template_new.xlsx')\n",
    "results_df.fillna(method='ffill', axis=0, inplace=True)\n",
    "results_df.set_index(['performance', 'model', 'trained on', 'seed'], inplace=True)\n",
    "\n",
    "for dataset_type in datasets:\n",
    "\n",
    "    df = pd.read_csv(f'./files_from_cluster/bert_{_[0]}_models.csv')\n",
    "    output_df = pd.DataFrame(columns=cols)\n",
    "    output_df2 = pd.DataFrame(columns=cols2)\n",
    "\n",
    "    for dataset_train in dataset_type[1]:\n",
    "        df_train_dataset = df[df['trained_on'] == dataset_train]\n",
    "        if dataset_type[0] == 'single':\n",
    "            data_on_train_set = df_train_dataset[df_train_dataset['predict_on'] == dataset_train]\n",
    "        elif dataset_type[0] == 'pair':\n",
    "            data_on_train_set = df_train_dataset[df_train_dataset['predict_on'] == dataset_train[:dataset_train.index('_')]]\n",
    "        best_accuracy_model = data_on_train_set[data_on_train_set['accuracy'] == data_on_train_set['accuracy'].max()].squeeze()\n",
    "        if type(best_accuracy_model) == pd.core.frame.DataFrame:\n",
    "            best_accuracy_model = best_accuracy_model.iloc[0]\n",
    "        seed = best_accuracy_model['seed']\n",
    "\n",
    "        # print(f'best accuracy of {dataset_train} is {best_accuracy_model.accuracy}, seed = {seed}')\n",
    "\n",
    "        for dataset_predict in single_datasets:\n",
    "            curr_df = df_train_dataset[df_train_dataset['predict_on'] == dataset_predict]\n",
    "            accs = curr_df['accuracy']\n",
    "            mean_acc, std_acc = np.mean(accs), np.std(accs)\n",
    "\n",
    "            df_with_seed = curr_df[curr_df['seed'] == seed].squeeze()\n",
    "            row_to_df = {'model': df_with_seed['model'], 'trained_on': df_with_seed['trained_on'],\n",
    "                         'seed': df_with_seed['seed'], 'predict_on': df_with_seed['predict_on'],\n",
    "                         'mean_accuracy': mean_acc, 'std_accuracy': std_acc,\n",
    "                         'accuracy': df_with_seed['accuracy'], 'recall': df_with_seed['recall'],\n",
    "                         'precision': df_with_seed['precision']}\n",
    "\n",
    "            for performance in ['accuracy', 'recall', 'precision']:\n",
    "                row_to_df2 = {'performance': performance, 'model': df_with_seed['model'],\n",
    "                            'trained_on': df_with_seed['trained_on'], 'seed': df_with_seed['seed']}\n",
    "                values = {dataset: df_with_seed[performance] for dataset in single_datasets}\n",
    "                row_to_df2.update(values)\n",
    "                # print(row_to_df2)\n",
    "                output_df2 = output_df2.append([row_to_df2])\n",
    "\n",
    "            output_df = output_df.append([row_to_df])\n",
    "\n",
    "        for performance in ['accuracy', 'recall', 'precision']:\n",
    "            row_to_df2 = {'performance': performance, 'model': df_with_seed['model'],\n",
    "                        'trained_on': df_with_seed['trained_on'], 'seed': df_with_seed['seed']}\n",
    "            values = {dataset: df_with_seed[performance] for dataset in single_datasets}\n",
    "            row_to_df2.update(values)\n",
    "            # print(row_to_df2)\n",
    "            output_df2 = output_df2.append([row_to_df2])\n",
    "\n",
    "    output_df.to_csv(f'./files_from_cluster/{_[0]}_models_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## write to final file (with correct format) from cluster results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "single_datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "paired_datasets = ['amazon_headlines', 'amazon_igg', 'amazon_twss', 'headlines_igg', 'headlines_twss', 'igg_twss']\n",
    "cols2 = ['performance', 'model', 'trained_on', 'seed', 'mean_accuracy', 'amazon', 'headlines', 'igg', 'twss']\n",
    "\n",
    "dataset_types = [('single', single_datasets), ('pair', paired_datasets)]\n",
    "dataset_types = [('pair', paired_datasets)]\n",
    "# dataset_types = [('single', single_datasets)]\n",
    "path = './files_from_drive/'\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    # df = pd.read_csv(f'./files_from_cluster/t5_{dataset_type[0]}_09-19.csv')\n",
    "    filename = 'bert_pair_09-19 - ep=4_bs=12'\n",
    "    df = pd.read_csv(path + filename + '.csv')\n",
    "    output_df2 = pd.DataFrame(columns=cols2)\n",
    "\n",
    "    for dataset_train in dataset_type[1]:\n",
    "        df_train_dataset = df[df['trained_on'] == dataset_train]\n",
    "        if dataset_type[0] == 'single':\n",
    "            data_on_train_set = df_train_dataset[df_train_dataset['predict_on'] == dataset_train]\n",
    "        elif dataset_type[0] == 'pair':\n",
    "            compute_acc_on = dataset_train[:dataset_train.index('_')]\n",
    "            if compute_acc_on == 'headlines':\n",
    "                compute_acc_on = dataset_train[dataset_train.index('_') + 1:]\n",
    "            data_on_train_set = df_train_dataset[\n",
    "                df_train_dataset['predict_on'] == compute_acc_on]\n",
    "        best_accuracy_model = data_on_train_set[\n",
    "            data_on_train_set['accuracy'] == data_on_train_set['accuracy'].max()].squeeze()\n",
    "        if type(best_accuracy_model) == pd.core.frame.DataFrame:\n",
    "            best_accuracy_model = best_accuracy_model.iloc[0]\n",
    "        seed = best_accuracy_model['seed']\n",
    "\n",
    "        df_with_seed = df_train_dataset[df_train_dataset['seed'] == seed]\n",
    "        accs_for_mean = data_on_train_set['accuracy']\n",
    "\n",
    "        for performance in ['accuracy', 'recall', 'precision']:\n",
    "            row_to_df2 = {'performance': performance, 'model': df_with_seed['model'].iloc[0],\n",
    "                          'trained_on': df_with_seed['trained_on'].iloc[0],\n",
    "                          'seed': df_with_seed['seed'].iloc[0],\n",
    "                          'mean_accuracy': f'{np.mean(accs_for_mean)} +- {np.std(accs_for_mean)}'}\n",
    "            values = {dataset: df_with_seed[df_with_seed['predict_on'] == dataset][performance].squeeze()\n",
    "                      for dataset in single_datasets}\n",
    "            row_to_df2.update(values)\n",
    "            output_df2 = output_df2.append([row_to_df2])\n",
    "\n",
    "    # output_df2.to_csv(f'./files_from_cluster/t5_{dataset_type[0]}_models_summary_09-19.csv', index=False)\n",
    "    output_df2.to_csv(path + filename + '_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## merger cluster files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "for model in ['t5', 'bert']:\n",
    "    df1 = pd.read_csv(f'./files_from_cluster/{model}_pair_09-19_1.csv')\n",
    "    df2 = pd.read_csv(f'./files_from_cluster/{model}_pair_09-19_2.csv')\n",
    "\n",
    "    df_merged = df1.append(df2, ignore_index=True)\n",
    "    df_merged.to_csv(f'./files_from_cluster/{model}_pair_09-19.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "single_datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "paired_datasets = ['amazon_headlines', 'amazon_igg', 'amazon_twss', 'headlines_igg', 'headlines_twss', 'igg_twss']\n",
    "cols2 = ['performance', 'model', 'trained_on', 'seed', 'mean_accuracy', 'amazon', 'headlines', 'igg', 'twss']\n",
    "\n",
    "dataset_types = [('single', single_datasets), ('pair', paired_datasets)]\n",
    "dataset_types = [('pair', paired_datasets)]\n",
    "# dataset_types = [('single', single_datasets)]\n",
    "path = './files_from_drive/'\n",
    "model_name ='t5_pair_09-19'\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    for ep in [3, 4]:\n",
    "        for bs in [4, 8, 12]:\n",
    "            # df = pd.read_csv(f'./files_from_cluster/t5_{dataset_type[0]}_09-19.csv')\n",
    "            filename = f'{model_name} - ep={ep}_bs={bs}'\n",
    "            df = pd.read_csv(path + filename + '.csv')\n",
    "            output_df2 = pd.DataFrame(columns=cols2)\n",
    "\n",
    "            for dataset_train in dataset_type[1]:\n",
    "                df_train_dataset = df[df['trained_on'] == dataset_train]\n",
    "                if dataset_type[0] == 'single':\n",
    "                    data_on_train_set = df_train_dataset[df_train_dataset['predict_on'] == dataset_train]\n",
    "                elif dataset_type[0] == 'pair':\n",
    "                    compute_acc_on = dataset_train[:dataset_train.index('_')]\n",
    "                    if compute_acc_on == 'headlines':\n",
    "                        compute_acc_on = dataset_train[dataset_train.index('_') + 1:]\n",
    "                    data_on_train_set = df_train_dataset[\n",
    "                        df_train_dataset['predict_on'] == compute_acc_on]\n",
    "                best_accuracy_model = data_on_train_set[\n",
    "                    data_on_train_set['accuracy'] == data_on_train_set['accuracy'].max()].squeeze()\n",
    "                if type(best_accuracy_model) == pd.core.frame.DataFrame:\n",
    "                    best_accuracy_model = best_accuracy_model.iloc[0]\n",
    "                seed = best_accuracy_model['seed']\n",
    "\n",
    "                df_with_seed = df_train_dataset[df_train_dataset['seed'] == seed]\n",
    "                accs_for_mean = data_on_train_set['accuracy']\n",
    "\n",
    "                for performance in ['accuracy', 'recall', 'precision']:\n",
    "                    row_to_df2 = {'performance': performance, 'model': df_with_seed['model'].iloc[0],\n",
    "                                  'trained_on': df_with_seed['trained_on'].iloc[0],\n",
    "                                  'seed': df_with_seed['seed'].iloc[0],\n",
    "                                  'mean_accuracy': f'{np.mean(accs_for_mean)} +- {np.std(accs_for_mean)}'}\n",
    "                    values = {dataset: df_with_seed[df_with_seed['predict_on'] == dataset][performance].squeeze()\n",
    "                              for dataset in single_datasets}\n",
    "                    row_to_df2.update(values)\n",
    "                    output_df2 = output_df2.append([row_to_df2])\n",
    "\n",
    "            # output_df2.to_csv(f'./files_from_cluster/t5_{dataset_type[0]}_models_summary_09-19.csv', index=False)\n",
    "            output_df2.to_csv(path + filename + '_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create dataframe and results summary from cluster results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## split amazon into sub dataset (amazonas) to check pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_path = './Data/humor_datasets/amazon/with_val_fixed_train/'\n",
    "amazon_test_df = pd.read_csv(path + 'test.csv')\n",
    "amazonas_dataset_df = amazon_test_df.iloc[int(len(amazon_test_df) / 2):]\n",
    "output_path = './Data/humor_datasets/amazonas/'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "amazonas_dataset_df.to_csv(output_path + 'data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## create with_val_fixed_train to amazonas\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fixed_train_size = 2376\n",
    "path = output_path + 'with_val_fixed_train'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "df = amazonas_dataset_df\n",
    "df_label_1_train = df[df.label == 1].iloc[:int(fixed_train_size / 2)]\n",
    "df_label_0_train = df[df.label == 0].iloc[:int(fixed_train_size / 2)]\n",
    "df_label_1_rest = df[df.label == 1].iloc[int(fixed_train_size / 2):]\n",
    "df_label_0_rest = df[df.label == 0].iloc[int(fixed_train_size / 2):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_label_0_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_train \u001B[38;5;241m=\u001B[39m \u001B[43mdf_label_0_train\u001B[49m\u001B[38;5;241m.\u001B[39mappend(df_label_1_train)\n\u001B[0;32m      2\u001B[0m df_train \u001B[38;5;241m=\u001B[39m df_train\u001B[38;5;241m.\u001B[39msample(frac\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      3\u001B[0m df_rest \u001B[38;5;241m=\u001B[39m df_label_0_rest\u001B[38;5;241m.\u001B[39mappend(df_label_1_rest)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df_label_0_train' is not defined"
     ]
    }
   ],
   "source": [
    "df_train = df_label_0_train.append(df_label_1_train)\n",
    "df_train = df_train.sample(frac=1, random_state=0, ignore_index=True)\n",
    "df_rest = df_label_0_rest.append(df_label_1_rest)\n",
    "df_rest = df_rest.sample(frac=1, random_state=0, ignore_index=True)\n",
    "\n",
    "df_test, df_val = train_test_split(df_rest, test_size=0.5, shuffle=True, random_state=0)\n",
    "df_test.to_csv(f'{path}/test.csv', index=False)\n",
    "df_train.to_csv(f'{path}/train.csv', index=False)\n",
    "df_val.to_csv(f'{path}/val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-20\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now().date())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def print_model_details(model_row):\n",
    "    print(f'ep = {model_row.epoch}, bs = {model_row.batch_size},'\n",
    "          f' lr = {model_row.learning_rate}, seed = {model_row.seed}')\n",
    "\n",
    "def print_model_details(epoch, batch_size, learning_rate, seed):\n",
    "    print(f'ep = {epoch}, bs = {batch_size},'\n",
    "          f' lr = {learning_rate}, seed = {seed}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** amazon ****\n",
      "best model is:\n",
      "ep = 3, bs = 8, lr = 0.0003, seed = 16\n",
      "with accuracy = 0.8949473684000001, f1 = 0.8927319722\n",
      "**** dadjokes ****\n",
      "best model is:\n",
      "ep = 3, bs = 8, lr = 0.0003, seed = 16\n",
      "with accuracy = 0.9375438596491229, f1 = 0.9354510943163215\n",
      "**** headlines ****\n",
      "best model is:\n",
      "ep = 3, bs = 8, lr = 0.0003, seed = 42\n",
      "with accuracy = 0.9229473684210527, f1 = 0.920086614137394\n",
      "**** one_liners ****\n",
      "best model is:\n",
      "ep = 3, bs = 8, lr = 0.0003, seed = 42\n",
      "with accuracy = 0.9245614033999999, f1 = 0.9249563278\n",
      "**** yelp_reviews ****\n",
      "best accuracy model:\n",
      "ep = 2, bs = 8, lr = 0.0003, seed = 42\n",
      "with accuracy = 0.7944561404000001\n",
      "best f1 model:\n",
      "ep = 3, bs = 8, lr = 0.0003, seed = 42\n",
      "with f1 = 0.7771602302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mortu\\AppData\\Local\\Temp\\ipykernel_17552\\3473204725.py:22: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  models = df.loc[(epoch, batch_size, learning_rate, seed)]\n",
      "C:\\Users\\mortu\\AppData\\Local\\Temp\\ipykernel_17552\\3473204725.py:22: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  models = df.loc[(epoch, batch_size, learning_rate, seed)]\n",
      "C:\\Users\\mortu\\AppData\\Local\\Temp\\ipykernel_17552\\3473204725.py:22: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  models = df.loc[(epoch, batch_size, learning_rate, seed)]\n",
      "C:\\Users\\mortu\\AppData\\Local\\Temp\\ipykernel_17552\\3473204725.py:22: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  models = df.loc[(epoch, batch_size, learning_rate, seed)]\n",
      "C:\\Users\\mortu\\AppData\\Local\\Temp\\ipykernel_17552\\3473204725.py:22: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  models = df.loc[(epoch, batch_size, learning_rate, seed)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "result_path = './files_from_cluster/'\n",
    "datasets = ['amazon', 'dadjokes', 'headlines', 'one_liners', 'yelp_reviews']\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f'**** {dataset} ****')\n",
    "    df = pd.read_csv(f'{result_path}{dataset}_scores.csv')\n",
    "    epochs = df['epoch'].unique()\n",
    "    batch_sizes = df['batch_size'].unique()\n",
    "    learning_rates =  df['learning_rate'].unique()\n",
    "    seeds = df['seed'].unique()\n",
    "    df = df.set_index(['epoch', 'batch_size', 'learning_rate', 'seed'])\n",
    "    best_accuracy, best_f1 = 0, 0\n",
    "    acc_params, f1_params = (), ()\n",
    "\n",
    "    # Iterate over all the combinations of hyperparameters\n",
    "    for epoch, batch_size, learning_rate, seed in  \\\n",
    "        itertools.product(epochs,batch_sizes,learning_rates, seeds):\n",
    "            models = df.loc[(epoch, batch_size, learning_rate, seed)]\n",
    "            avg_accuracy = models['accuracy'].mean()\n",
    "            avg_f1 = models['f1'].mean()\n",
    "\n",
    "            if avg_accuracy > best_accuracy:\n",
    "                best_accuracy = avg_accuracy\n",
    "                acc_params = epoch, batch_size, learning_rate, seed\n",
    "\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1 = avg_f1\n",
    "                f1_params = epoch, batch_size, learning_rate, seed\n",
    "\n",
    "    if (acc_params == f1_params):\n",
    "        print('best model is:')\n",
    "        print_model_details(*acc_params)\n",
    "        print(f'with accuracy = {best_accuracy}, f1 = {best_f1}')\n",
    "    else:\n",
    "        print('best accuracy model:')\n",
    "        print_model_details(*acc_params)\n",
    "        print(f'with accuracy = {best_accuracy}')\n",
    "\n",
    "        print('best f1 model:')\n",
    "        print_model_details(*f1_params)\n",
    "        print(f'with f1 = {best_f1}')\n",
    "\n",
    "    # sorted_df_acc = df.sort_values(by='accuracy', ascending=False)\n",
    "    # sorted_df_f1 = df.sort_values(by='f1', ascending=False)\n",
    "    # best_acc_row = sorted_df_acc.iloc[0]\n",
    "    # best_f1_row = sorted_df_f1.iloc[0]\n",
    "    # if (best_acc_row == best_f1_row).all():\n",
    "    #     print('best model is:')\n",
    "    #     print_model_details(best_acc_row)\n",
    "    # else:\n",
    "    #     print('best accuracy model:')\n",
    "    #     print_model_details(best_acc_row)\n",
    "    #     print('best f1 model:')\n",
    "    #     print_model_details(best_f1_row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "0     flan-t5-base-yelp_reviews-text-classification-...\n1     flan-t5-base-yelp_reviews-text-classification-...\n2     flan-t5-base-yelp_reviews-text-classification-...\n3     flan-t5-base-yelp_reviews-text-classification-...\n4     flan-t5-base-yelp_reviews-text-classification-...\n5     flan-t5-base-yelp_reviews-text-classification-...\n6     flan-t5-base-yelp_reviews-text-classification-...\n7     flan-t5-base-yelp_reviews-text-classification-...\n8     flan-t5-base-yelp_reviews-text-classification-...\n9     flan-t5-base-yelp_reviews-text-classification-...\n10    flan-t5-base-yelp_reviews-text-classification-...\n11    flan-t5-base-yelp_reviews-text-classification-...\n12    flan-t5-base-yelp_reviews-text-classification-...\n13    flan-t5-base-yelp_reviews-text-classification-...\n14    flan-t5-base-yelp_reviews-text-classification-...\n15    flan-t5-base-yelp_reviews-text-classification-...\n16    flan-t5-base-yelp_reviews-text-classification-...\n17    flan-t5-base-yelp_reviews-text-classification-...\n18    flan-t5-base-yelp_reviews-text-classification-...\n19    flan-t5-base-yelp_reviews-text-classification-...\n20    flan-t5-base-yelp_reviews-text-classification-...\n21    flan-t5-base-yelp_reviews-text-classification-...\n22    flan-t5-base-yelp_reviews-text-classification-...\n23    flan-t5-base-yelp_reviews-text-classification-...\n24    flan-t5-base-yelp_reviews-text-classification-...\n25    flan-t5-base-yelp_reviews-text-classification-...\n26    flan-t5-base-yelp_reviews-text-classification-...\n27    flan-t5-base-yelp_reviews-text-classification-...\n28    flan-t5-base-yelp_reviews-text-classification-...\n29    flan-t5-base-yelp_reviews-text-classification-...\n30    flan-t5-base-yelp_reviews-text-classification-...\n31    flan-t5-base-yelp_reviews-text-classification-...\n32    flan-t5-base-yelp_reviews-text-classification-...\n33    flan-t5-base-yelp_reviews-text-classification-...\n34    flan-t5-base-yelp_reviews-text-classification-...\n35    flan-t5-base-yelp_reviews-text-classification-...\n36    flan-t5-base-yelp_reviews-text-classification-...\n37    flan-t5-base-yelp_reviews-text-classification-...\n38    flan-t5-base-yelp_reviews-text-classification-...\n39    flan-t5-base-yelp_reviews-text-classification-...\nName: model, dtype: object"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df_multi_index = df.set_index(['epoch', 'batch_size', 'learning_rate', 'seed'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mortu\\AppData\\Local\\Temp\\ipykernel_17552\\52637515.py:1: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  specific_row = df_multi_index.loc[(2, 8, 0.0003, 16)]\n"
     ]
    }
   ],
   "source": [
    "specific_row = df_multi_index.loc[(2, 8, 0.0003, 16)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in df['split'].unique():\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    # BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "MODEL_ID = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': ['[SEP]']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.sep_token)\n",
    "# tokenizer.sep_token = '[SEP]'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['</s>', '<unk>', '<pad>', '[SEP]']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 2, 0, 32100]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, Dataset\n",
    "dataset1 = Dataset.from_dict({'text': ['example1', 'example2'], 'label': [0, 1]})\n",
    "dataset2 = Dataset.from_dict({'text': ['example3', 'example4'], 'label': [1, 0]})\n",
    "dataset3 = Dataset.from_dict({'text': ['example5', 'example6'], 'label': [0, 1]})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "combined = Dataset.from_dict({'text': [], 'label': []})\n",
    "combined = concatenate_datasets([combined, dataset1])\n",
    "combined_dataset = concatenate_datasets([dataset1, dataset2, dataset3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df = combined_dataset.to_pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "shuffled_dataset = combined_dataset.shuffle(seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df_shuffle = shuffled_dataset.to_pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = './Data/new_humor_datasets/balanced/headlines/data.csv'\n",
    "df = pd.read_csv(path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "percent = 0.07\n",
    "if percent and type(percent) is float and percent <= 100:\n",
    "    samples_count = int(len(df) * percent / 100)\n",
    "    df = df.iloc[:samples_count]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Generator\n",
    "def gen_natural(k):\n",
    "    while True:\n",
    "        yield k\n",
    "        k += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "my_gen = gen_natural(3)\n",
    "print(type(my_gen))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humor_venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}