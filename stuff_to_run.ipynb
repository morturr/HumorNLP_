{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## create predictions file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "{(1, 2, 3, 5): 6, (2, 3, 3, 6): 8}"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = [3, 3]\n",
    "batch_sizes = [8]\n",
    "lrs = [5e-5, 1e-6, 1e-5]  # [5e-5, 1e-6]\n",
    "seeds = [42]\n",
    "\n",
    "results = {}\n",
    "results[1,2, 3, 5] = 6\n",
    "results[2, 3, 3, 6] = 8\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "results_file_path = 'Data/output/results/{model_name}_on_{dataset}_{date}'.format(\n",
    "    model_name='t5',\n",
    "    dataset='igg',\n",
    "    date='23_08_2023'\n",
    ")\n",
    "\n",
    "with open(results_file_path, 'a') as f:\n",
    "    for k, v in results.items():\n",
    "        f.write(f'ep: {k[0]}, bs: {k[1]}, lr: {k[2]}, seed: {k[3]}\\n')\n",
    "        f.write(f'accuracy = {v}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# find smallest test size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size of amazon is 8359\n",
      "test size of headlines is 5150\n",
      "test size of igg is 519\n",
      "test size of twss is 788\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/test.csv'\n",
    "test_size = None\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    print(f'test size of {dataset} is {len(df)}')\n",
    "test_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size of amazon is 2376\n",
      "label 1 count is 1188\n",
      "train size of headlines is 2376\n",
      "label 1 count is 1188\n",
      "train size of igg is 2376\n",
      "label 1 count is 1188\n",
      "train size of twss is 2376\n",
      "label 1 count is 1188\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/train.csv'\n",
    "train_size = None\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    print(f'train size of {dataset} is {len(df)}')\n",
    "    print(f'label 1 count is {len(df[df.label==1])}')\n",
    "train_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val size of amazon is 8359\n",
      "val size of headlines is 5150\n",
      "val size of igg is 519\n",
      "val size of twss is 788\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/val.csv'\n",
    "val_size = None\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    print(f'val size of {dataset} is {len(df)}')\n",
    "val_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for amazon:\n",
      "%label 1 = 50.77, %label 0 = 49.23\n",
      "for headlines:\n",
      "%label 1 = 48.86, %label 0 = 51.14\n",
      "for igg:\n",
      "%label 1 = 52.79, %label 0 = 47.21\n",
      "for twss:\n",
      "%label 1 = 47.84, %label 0 = 52.16\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/test.csv'\n",
    "max_test_size = 3500\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    df = df.iloc[:min(len(df), max_test_size)]\n",
    "    df_1 = df[df.label == 1]\n",
    "    df_0 = df[df.label == 0]\n",
    "\n",
    "    # print(f'test size of {dataset} is {len(df)}')\n",
    "    print(f'for {dataset}:')\n",
    "    print(f'%label 1 = {\"%.2f\" % (100 * len(df_1) / len(df))}, %label 0 = {\"%.2f\" % (100 * len(df_0) / len(df))}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for amazon:\n",
      "%label 1 = 49.46, %label 0 = 50.54\n",
      "for headlines:\n",
      "%label 1 = 47.11, %label 0 = 52.89\n",
      "for igg:\n",
      "%label 1 = 47.21, %label 0 = 52.79\n",
      "for twss:\n",
      "%label 1 = 49.11, %label 0 = 50.89\n"
     ]
    }
   ],
   "source": [
    "path = './Data/humor_datasets/{dataset}/with_val_fixed_train/val.csv'\n",
    "max_val_size = 3500\n",
    "datasets = ['amazon', 'headlines', 'igg', 'twss']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(path.format(dataset=dataset))\n",
    "    df = df.iloc[:min(len(df), max_val_size)]\n",
    "    df_1 = df[df.label == 1]\n",
    "    df_0 = df[df.label == 0]\n",
    "\n",
    "    print(f'for {dataset}:')\n",
    "    print(f'%label 1 = {\"%.2f\" % (100 * len(df_1) / len(df))}, %label 0 = {\"%.2f\" % (100 * len(df_0) / len(df))}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# compute performance of the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_run_details(run_name):\n",
    "    run_data = run_name.split('_')\n",
    "    model = run_data[0]\n",
    "    dataset_name = run_data[2]\n",
    "    seed = run_data[3][run_data[3].index('=') + 1:]\n",
    "\n",
    "    # return model, dataset_name, float(seed)\n",
    "    return dataset_name, float(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Model/SavedModels/T5\\\\t5-base_on_amazon_seed=42', './Model/SavedModels/T5\\\\t5-base_on_headlines_seed=5', './Model/SavedModels/T5\\\\t5-base_on_igg_seed=18', './Model/SavedModels/T5\\\\t5-base_on_twss_seed=42']\n",
      "performance for ./Model/SavedModels/T5\\t5-base_on_amazon_seed=42\n",
      "accuracies = {'amazon': 0.8554, 'headlines': 0.502, 'igg': 0.6378, 'twss': 0.3414}\n",
      "recall = {'amazon': 0.8436, 'headlines': 0.9766, 'igg': 0.8723, 'twss': 0.687}\n",
      "precision = {'amazon': 0.868, 'headlines': 0.4951, 'igg': 0.6097, 'twss': 0.3924}\n",
      "performance for ./Model/SavedModels/T5\\t5-base_on_headlines_seed=5\n",
      "accuracies = {'amazon': 0.5589, 'headlines': 0.5831, 'igg': 0.6936, 'twss': 0.4746}\n",
      "recall = {'amazon': 0.2662, 'headlines': 0.8304, 'igg': 0.6934, 'twss': 0.9337}\n",
      "precision = {'amazon': 0.6634, 'headlines': 0.5485, 'igg': 0.717, 'twss': 0.475}\n",
      "performance for ./Model/SavedModels/T5\\t5-base_on_igg_seed=18\n",
      "accuracies = {'amazon': 0.5406, 'headlines': 0.5023, 'igg': 0.9268, 'twss': 0.4797}\n",
      "recall = {'amazon': 0.9741, 'headlines': 0.9819, 'igg': 0.9416, 'twss': 0.9973}\n",
      "precision = {'amazon': 0.5257, 'headlines': 0.4953, 'igg': 0.9214, 'twss': 0.479}\n",
      "there are 16 illegal indices in twss predictions on amazon\n",
      "there are 25 illegal indices in twss predictions on headlines\n",
      "there are 12 illegal indices in twss predictions on igg\n",
      "performance for ./Model/SavedModels/T5\\t5-base_on_twss_seed=42\n",
      "accuracies = {'amazon': 0.5132, 'headlines': 0.4918, 'igg': 0.5444, 'twss': 0.7995}\n",
      "recall = {'amazon': 0.9518, 'headlines': 0.93, 'igg': 0.8502, 'twss': 1.0}\n",
      "precision = {'amazon': 0.5103, 'headlines': 0.4874, 'igg': 0.5431, 'twss': 0.7047}\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "import glob\n",
    "import os\n",
    "\n",
    "output_path = './Data/output/results/'\n",
    "dataset_names = ['amazon', 'headlines', 'igg', 'twss']\n",
    "data_path = './Data/humor_datasets/'\n",
    "split_type = 'with_val_fixed_train'\n",
    "models_path = './Model/SavedModels/T5'\n",
    "base_model = 't5-base'\n",
    "models_name = [glob.glob(f'{models_path}/{base_model}_on_{dataset}*')[0] for dataset in dataset_names]\n",
    "\n",
    "print(models_name)\n",
    "df = pd.read_excel(output_path + 'humor_results_template.xlsx')\n",
    "df.fillna(method='ffill', axis=0, inplace=True)\n",
    "df.set_index(['performance', 'model', 'trained on', 'seed'], inplace=True)\n",
    "\n",
    "for model_name in models_name:\n",
    "    # base_model, dataset_name, seed = get_run_details(model_name)\n",
    "    dataset_name, seed = get_run_details(model_name)\n",
    "    pred_path = model_name + '/predictions/'\n",
    "    accuracies = {}\n",
    "    recall = {}\n",
    "    precision = {}\n",
    "    for dataset in dataset_names:\n",
    "        pred_labels_path = pred_path + f'{dataset}_preds.csv'\n",
    "        test_labels_path = data_path + f'{dataset}/{split_type}/test.csv'\n",
    "        if not (exists(pred_labels_path) and exists(test_labels_path)):\n",
    "            print(pred_labels_path)\n",
    "            print('didnt find preds/test path')\n",
    "            continue\n",
    "\n",
    "        _preds = pd.read_csv(pred_labels_path)\n",
    "        _test = pd.read_csv(test_labels_path)\n",
    "        _test = _test.iloc[:len(_preds)]\n",
    "        if (len(_preds[_preds.label == -1]) > 0):\n",
    "            illegal_indices = _preds[_preds.label == -1].index\n",
    "            print(f'there are {len(illegal_indices)} illegal indices in {dataset_name} predictions on {dataset}')\n",
    "            _preds = _preds.drop(labels=illegal_indices, axis=0)\n",
    "            _test = _test.drop(labels=illegal_indices, axis=0)\n",
    "        accuracies[dataset] = float(\"%.4f\" % accuracy_score(_test.label, _preds.label))\n",
    "        recall[dataset] = float(\"%.4f\" % recall_score(_test.label, _preds.label))\n",
    "        precision[dataset] = float(\"%.4f\" % precision_score(_test.label, _preds.label))\n",
    "\n",
    "    print(f'performance for {model_name}')\n",
    "    print(f'accuracies = {accuracies}')\n",
    "    print(f'recall = {recall}')\n",
    "    print(f'precision = {precision}')\n",
    "\n",
    "    df.loc[('accuracy', base_model, dataset_name, seed)] = accuracies\n",
    "    df.loc[('recall', base_model, dataset_name, seed)] = recall\n",
    "    df.loc[('precision', base_model, dataset_name, seed)] = precision\n",
    "\n",
    "# save performance to output file\n",
    "i = 0\n",
    "while os.path.exists(output_path + f'humor_results_{i}.xlsx'):\n",
    "    i += 1\n",
    "\n",
    "df.to_excel(output_path + f'humor_results_{i}.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# compute models mean & std accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon: 0.8551 +- 0.001\n",
      "headlines: 0.5816 +- 0.001\n",
      "igg: 0.9370 +- 0.002\n",
      "twss: 0.5555 +- 0.141\n"
     ]
    }
   ],
   "source": [
    "acc_igg = [0.9347826086956522, 0.9391304347826087, 0.9376811594202898, 0.936231884057971]\n",
    "acc_amazon = [0.8557142857142858, 0.8542857142857143, 0.8551428571428571, 0.8554285714285714]\n",
    "acc_headlines = [0.5831428571428572, 0.5805714285714285, 0.5822857142857143, 0.5805714285714285]\n",
    "acc_twss = [0.45634920634920634, 0.4777636594663278, 0.4885786802030457, 0.799492385786802]\n",
    "accs = {'amazon': acc_amazon, 'headlines': acc_headlines, 'igg': acc_igg, 'twss': acc_twss}\n",
    "for k,v in accs.items():\n",
    "    print(f'{k}: {\"%.4f\" % np.mean(v)} +- {\"%.3f\" % np.std(v)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}