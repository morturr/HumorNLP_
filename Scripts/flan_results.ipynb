{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "USE_MODEL = 'Llama-2'\n",
    "PATH = f'../{USE_MODEL}/Results'\n",
    "\n",
    "DATASETS = ['amazon', 'dadjokes', 'headlines', 'one_liners', 'yelp_reviews']\n",
    "# DATASETS = ['amazon']\n",
    "PATH_TEMPLATE = PATH + '/{dataset}-models/'\n",
    "TABLES_PATH = PATH + '/Tables/'\n",
    "\n",
    "LOO_DATASETS = [f'loo_{dataset}' for dataset in DATASETS]\n",
    "LOO_PATH = PATH + '/leave-one-out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# LOO_EVAL = False\n",
    "LOO_EVAL = False\n",
    "\n",
    "TRAIN_DATASETS = LOO_DATASETS if LOO_EVAL else DATASETS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mortu\\AppData\\Local\\Temp\\ipykernel_6436\\636092915.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  scores_df = pd.concat([scores_df, df])\n"
     ]
    }
   ],
   "source": [
    "# Create the score table for regular dataset\n",
    "if not LOO_EVAL:\n",
    "    scores_df = pd.DataFrame()\n",
    "    for dataset_name in TRAIN_DATASETS:\n",
    "        root_dir = PATH_TEMPLATE.format(dataset=dataset_name)\n",
    "        for _, dirs, _ in os.walk(root_dir):\n",
    "            for dir_name in dirs:\n",
    "                inner_dir = os.path.join(root_dir, dir_name)\n",
    "                score_file_path = os.path.join(inner_dir, f'{dataset_name}_scores.csv')\n",
    "                df = pd.read_csv(score_file_path)\n",
    "                # for the first df, set its columns to be the columns of the overall dataframe (scores_df)\n",
    "                if scores_df.empty:\n",
    "                    scores_df = pd.DataFrame(columns=df.columns)\n",
    "                scores_df = pd.concat([scores_df, df])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Create the score table for loo dataset (only one model per dataset)\n",
    "if LOO_EVAL:\n",
    "    scores_df = pd.DataFrame()\n",
    "    for dataset_name in TRAIN_DATASETS:\n",
    "        score_file_path = os.path.join(LOO_PATH, f'{dataset_name}_scores.csv')\n",
    "        df = pd.read_csv(score_file_path)\n",
    "        # for the first df, set its columns to be the columns of the overall dataframe (scores_df)\n",
    "        if scores_df.empty:\n",
    "            scores_df = pd.DataFrame(columns=df.columns)\n",
    "        scores_df = pd.concat([scores_df, df])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mortu\\AppData\\Local\\Temp\\ipykernel_6436\\1495392005.py:8: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result_df.fillna(method='ffill', axis=0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "METRICS = ['accuracy', 'f1', 'recall', 'precision']\n",
    "base_model = 'flan-t5-base'\n",
    "base_model = 'llama-2-7b'\n",
    "base_model = 'mistral-7b'\n",
    "# models_name = [glob.glob(f'{models_path}/{base_model}_on_{dataset}*')[0] for dataset in dataset_names]\n",
    "\n",
    "result_df = pd.read_excel(TABLES_PATH + 'result_template.xlsx')\n",
    "result_df.fillna(method='ffill', axis=0, inplace=True)\n",
    "result_df.set_index(['metric', 'model', 'trained on'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'METRICS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# calculate metrics' mean and std for all pairs of datasets\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m train_dataset \u001B[38;5;129;01min\u001B[39;00m TRAIN_DATASETS:\n\u001B[1;32m----> 3\u001B[0m     metrics_dict \u001B[38;5;241m=\u001B[39m {metric_name: {} \u001B[38;5;28;01mfor\u001B[39;00m metric_name \u001B[38;5;129;01min\u001B[39;00m \u001B[43mMETRICS\u001B[49m}\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m eval_dataset \u001B[38;5;129;01min\u001B[39;00m DATASETS:\n\u001B[0;32m      6\u001B[0m         df \u001B[38;5;241m=\u001B[39m scores_df[(scores_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_dataset\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m train_dataset) \u001B[38;5;241m&\u001B[39m (scores_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mevaluate_dataset\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m eval_dataset)]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'METRICS' is not defined"
     ]
    }
   ],
   "source": [
    "# calculate metrics' mean and std for all pairs of datasets\n",
    "for train_dataset in TRAIN_DATASETS:\n",
    "    metrics_dict = {metric_name: {} for metric_name in METRICS}\n",
    "\n",
    "    for eval_dataset in DATASETS:\n",
    "        df = scores_df[(scores_df['train_dataset'] == train_dataset) & (scores_df['evaluate_dataset'] == eval_dataset)]\n",
    "        # print(train_dataset, eval_dataset)\n",
    "        for metric in METRICS:\n",
    "            # print(metric)\n",
    "            values = df[metric]\n",
    "            mean, std = values.mean(), values.std()\n",
    "            metrics_dict[metric][eval_dataset] = float(\"%.4f\" % mean)\n",
    "            # print(mean, std)\n",
    "\n",
    "    for metric in METRICS:\n",
    "        result_df.loc[(metric, base_model, train_dataset)] = metrics_dict[metric]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# save performance to output file\n",
    "date = datetime.now().date()\n",
    "i = 1\n",
    "while os.path.exists(TABLES_PATH + f'humor_results_{date}_{i}*.xlsx'):\n",
    "    i += 1\n",
    "\n",
    "result_df.to_excel(TABLES_PATH + f'humor_results_{date}_{i}.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create table by best median/mean/acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def find_best_accuracies(df, trained_dataset_name, which_best='All'):\n",
    "    accuracies = {}\n",
    "    best_trained, best_median, best_mean = (-1, None), (-1, None), (-1, None)\n",
    "\n",
    "    for params, rows in df.groupby(by=params_column_names):  # level=0 refers to 'letter'\n",
    "        # print(f\"Index: {name}\")\n",
    "        # print(group)\n",
    "        row_trained = rows[rows['evaluate_dataset'] == trained_dataset_name]\n",
    "        row_others = rows[rows['evaluate_dataset'] != trained_dataset_name]\n",
    "        curr_trained_accuracy = row_trained['accuracy'].values[0]\n",
    "        others_accuracy_median = row_others['accuracy'].median()\n",
    "        others_accuracy_mean = row_others['accuracy'].mean()\n",
    "\n",
    "        best_trained = (curr_trained_accuracy, params) if curr_trained_accuracy > best_trained[0] else best_trained\n",
    "        best_median = (others_accuracy_median, params) if others_accuracy_median > best_median[0] else best_median\n",
    "        best_mean = (others_accuracy_mean, params) if others_accuracy_mean > best_mean[0] else best_mean\n",
    "\n",
    "\n",
    "        curr_accs = {'trained': curr_trained_accuracy,\n",
    "                     'median': others_accuracy_median,\n",
    "                     'mean': others_accuracy_mean}\n",
    "\n",
    "        accuracies[params] = curr_accs\n",
    "\n",
    "    # print(f'Accuracies for dataset: {trained_dataset_name}')\n",
    "    # print(f'Best trained accuracy = {best_trained}')\n",
    "    # print(f'Best median accuracy = {best_median}')\n",
    "    # print(f'Best mean accuracy = {best_mean}')\n",
    "\n",
    "    if which_best == 'All':\n",
    "        return {'best_trained': best_trained,\n",
    "                'best_median': best_median,\n",
    "                'best_mean': best_mean}\n",
    "\n",
    "    # Return only best median\n",
    "    if which_best == 'Median':\n",
    "        return {'best_median': best_median}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "params_column_names = ['seed', 'learning_rate', 'per_device_train_batch_size',\n",
    "       'per_device_eval_batch_size', 'max_steps', 'lora_rank', 'lora_alpha',]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mortu\\AppData\\Local\\Temp\\ipykernel_6436\\817679394.py:8: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result_param_df.fillna(method='ffill', axis=0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "METRICS = ['accuracy', 'f1', 'recall', 'precision']\n",
    "# base_model = 'flan-t5-base'\n",
    "base_model = 'llama-2-7b'\n",
    "# base_model = 'mistral-7b'\n",
    "# models_name = [glob.glob(f'{models_path}/{base_model}_on_{dataset}*')[0] for dataset in dataset_names]\n",
    "\n",
    "result_param_df = pd.read_excel(TABLES_PATH + 'result_param_metric_template.xlsx')\n",
    "result_param_df.fillna(method='ffill', axis=0, inplace=True)\n",
    "result_param_df.set_index(['metric', 'model', 'trained on', 'param_metric'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def get_split_avg_df(df):\n",
    "    # Iterate over the existing parameters combinations and create df of the average cv splits results\n",
    "    df_avg_splits = pd.DataFrame(columns=df.columns)\n",
    "    for params, rows in df.groupby(by=params_column_names):\n",
    "        for dataset_name in DATASETS:\n",
    "            rows_of_dataset = rows[rows['evaluate_dataset'] == dataset_name]\n",
    "\n",
    "            # Columns you want to average\n",
    "            columns_to_average = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "            # Calculate the mean for the relevant columns\n",
    "            mean_values = rows_of_dataset[columns_to_average].mean()\n",
    "\n",
    "            # Prepare a new row with average values and other relevant info\n",
    "            # For example, 'split' can be labeled as 'average'\n",
    "            new_row = rows_of_dataset.iloc[0].copy(deep=True)\n",
    "\n",
    "            new_row.update(mean_values)\n",
    "            new_row.update({'split_num': 'average'})\n",
    "            new_row_df = pd.DataFrame([new_row], columns=df.columns)\n",
    "\n",
    "            df_avg_splits = pd.concat([df_avg_splits, new_row_df], axis=0)\n",
    "\n",
    "    return df_avg_splits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get dataframe of the averages of the cv splits:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mortu\\AppData\\Local\\Temp\\ipykernel_6436\\4069700627.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_avg_splits = pd.concat([df_avg_splits, new_row_df], axis=0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of new names must be 1, got 7",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[82], line 15\u001B[0m\n\u001B[0;32m     12\u001B[0m df_curr_trained_avg_splits \u001B[38;5;241m=\u001B[39m get_split_avg_df(df_curr_trained)\n\u001B[0;32m     13\u001B[0m df_avg_splits_all \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([df_curr_trained_avg_splits, df_avg_splits_all])\n\u001B[1;32m---> 15\u001B[0m \u001B[43mdf_avg_splits_all\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnames\u001B[49m \u001B[38;5;241m=\u001B[39m df_curr_trained\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39mnames\n",
      "File \u001B[1;32m~\\PycharmProjects\\HumorNLP_\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:1787\u001B[0m, in \u001B[0;36mIndex._set_names\u001B[1;34m(self, values, level)\u001B[0m\n\u001B[0;32m   1785\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNames must be a list-like\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1786\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(values) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m-> 1787\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLength of new names must be 1, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(values)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1789\u001B[0m \u001B[38;5;66;03m# GH 20527\u001B[39;00m\n\u001B[0;32m   1790\u001B[0m \u001B[38;5;66;03m# All items in 'name' need to be hashable:\u001B[39;00m\n\u001B[0;32m   1791\u001B[0m validate_all_hashable(\u001B[38;5;241m*\u001B[39mvalues, error_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.name\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Length of new names must be 1, got 7"
     ]
    }
   ],
   "source": [
    "df_avg_splits_all = pd.DataFrame()\n",
    "for dataset_name in TRAIN_DATASETS:\n",
    "    # For now taking only the first split (until I'll have all the splits results)\n",
    "    # and then I'll need to average on the splits\n",
    "    df_curr_trained = scores_df[(scores_df['train_dataset'] == dataset_name)]\n",
    "\n",
    "\n",
    "    params_vals = {param_name: list(df_curr_trained[param_name].unique()) for param_name in params_column_names}\n",
    "\n",
    "    df_curr_trained.set_index(params_column_names, inplace=True)\n",
    "\n",
    "    df_curr_trained_avg_splits = get_split_avg_df(df_curr_trained)\n",
    "    df_avg_splits_all = pd.concat([df_curr_trained_avg_splits, df_avg_splits_all])\n",
    "\n",
    "    df_avg_splits_all.index.names = df_curr_trained.index.names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "7"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_curr_trained.index.names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "Index([   (42, 5e-06, 2, 2, 150, 32, 8),    (42, 5e-06, 2, 2, 150, 32, 8),\n          (42, 5e-06, 2, 2, 150, 32, 8),    (42, 5e-06, 2, 2, 150, 32, 8),\n          (42, 5e-06, 2, 2, 150, 32, 8),   (42, 5e-06, 2, 2, 150, 32, 16),\n         (42, 5e-06, 2, 2, 150, 32, 16),   (42, 5e-06, 2, 2, 150, 32, 16),\n         (42, 5e-06, 2, 2, 150, 32, 16),   (42, 5e-06, 2, 2, 150, 32, 16),\n       ...\n        (42, 0.0003, 2, 2, 200, 128, 8),  (42, 0.0003, 2, 2, 200, 128, 8),\n        (42, 0.0003, 2, 2, 200, 128, 8),  (42, 0.0003, 2, 2, 200, 128, 8),\n        (42, 0.0003, 2, 2, 200, 128, 8), (42, 0.0003, 2, 2, 200, 128, 64),\n       (42, 0.0003, 2, 2, 200, 128, 64), (42, 0.0003, 2, 2, 200, 128, 64),\n       (42, 0.0003, 2, 2, 200, 128, 64), (42, 0.0003, 2, 2, 200, 128, 64)],\n      dtype='object', length=225)"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_avg_splits_all.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "MultiIndex([(42, 0.0003, 2, 2, 200,  64, 32),\n            (42, 0.0003, 2, 2, 200,  64, 32),\n            (42, 0.0003, 2, 2, 200,  64, 32),\n            (42, 0.0003, 2, 2, 200,  64, 32),\n            (42, 0.0003, 2, 2, 200,  64, 32),\n            (42,  5e-05, 2, 2, 150, 128, 16),\n            (42,  5e-05, 2, 2, 150, 128, 16),\n            (42,  5e-05, 2, 2, 150, 128, 16),\n            (42,  5e-05, 2, 2, 150, 128, 16),\n            (42,  5e-05, 2, 2, 150, 128, 16),\n            ...\n            (42, 0.0003, 2, 2, 200,  64,  8),\n            (42, 0.0003, 2, 2, 200,  64,  8),\n            (42, 0.0003, 2, 2, 200,  64,  8),\n            (42, 0.0003, 2, 2, 200,  64,  8),\n            (42, 0.0003, 2, 2, 200,  64,  8),\n            (42,  1e-05, 2, 2, 200, 128,  8),\n            (42,  1e-05, 2, 2, 200, 128,  8),\n            (42,  1e-05, 2, 2, 200, 128,  8),\n            (42,  1e-05, 2, 2, 200, 128,  8),\n            (42,  1e-05, 2, 2, 200, 128,  8)],\n           names=['seed', 'learning_rate', 'per_device_train_batch_size', 'per_device_eval_batch_size', 'max_steps', 'lora_rank', 'lora_alpha'], length=900)"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_curr_trained.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seed', 'learning_rate', 'per_device_train_batch_size', 'per_device_eval_batch_size', 'max_steps', 'lora_rank', 'lora_alpha']\n"
     ]
    }
   ],
   "source": [
    "# Convert the single-level index into a MultiIndex\n",
    "df_avg_splits_all.index = pd.MultiIndex.from_tuples(df_avg_splits_all.index, names=df_curr_trained.index.names)\n",
    "\n",
    "# Now df_avg_splits_all will have a MultiIndex with 7 levels\n",
    "print(df_avg_splits_all.index.names)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['seed', 'learning_rate', 'per_device_train_batch_size', 'per_device_eval_batch_size', 'max_steps', 'lora_rank', 'lora_alpha'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_6436\\2136128832.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;31m# and then I'll need to average on the splits\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mdf_curr_trained\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf_avg_splits_all\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_avg_splits_all\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'train_dataset'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mdataset_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[1;31m# params_vals = {param_name: list(df_curr_trained[param_name].unique()) for param_name in params_column_names}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m     \u001B[0mdf_curr_trained\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparams_column_names\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minplace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[0mbest_accs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfind_best_accuracies\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_curr_trained\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdataset_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwhich_best\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'Median'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[0mdataset_accs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdataset_name\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbest_accs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\HumorNLP_\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001B[0m\n\u001B[0;32m   6105\u001B[0m                     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mfound\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6106\u001B[0m                         \u001B[0mmissing\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6107\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6108\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mmissing\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 6109\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"None of {missing} are in the columns\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   6110\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6111\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0minplace\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6112\u001B[0m             \u001B[0mframe\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: \"None of ['seed', 'learning_rate', 'per_device_train_batch_size', 'per_device_eval_batch_size', 'max_steps', 'lora_rank', 'lora_alpha'] are in the columns\""
     ]
    }
   ],
   "source": [
    "dataset_accs = {}\n",
    "for dataset_name in TRAIN_DATASETS:\n",
    "    # For now taking only the first split (until I'll have all the splits results)\n",
    "    # and then I'll need to average on the splits\n",
    "    df_curr_trained = df_avg_splits_all[(df_avg_splits_all['train_dataset'] == dataset_name)]\n",
    "\n",
    "    # params_vals = {param_name: list(df_curr_trained[param_name].unique()) for param_name in params_column_names}\n",
    "\n",
    "    df_curr_trained.set_index(params_column_names, inplace=True)\n",
    "    best_accs = find_best_accuracies(df_curr_trained, dataset_name, which_best='Median')\n",
    "    dataset_accs[dataset_name] = best_accs\n",
    "    # Iterate over the best accuracies and save all of them\n",
    "    # calculate metrics' mean and std for all pairs of datasets\n",
    "    for param_mertic, acc_and_params in best_accs.items():\n",
    "        params = acc_and_params[1]\n",
    "        df_params = df_curr_trained.at[params]\n",
    "        metrics_dict = {metric_name: {} for metric_name in METRICS}\n",
    "\n",
    "        for eval_dataset in DATASETS:\n",
    "            df = df_params[(df_params['evaluate_dataset'] == eval_dataset)]\n",
    "            # print(train_dataset, eval_dataset)\n",
    "            for metric in METRICS:\n",
    "                # print(metric)\n",
    "                values = df[metric]\n",
    "                mean, std = values.mean(), values.std()\n",
    "                metrics_dict[metric][eval_dataset] = float(\"%.4f\" % mean)\n",
    "                # print(mean, std)\n",
    "\n",
    "        for metric in METRICS:\n",
    "            result_param_df.loc[(metric, base_model, dataset_name, param_mertic)] = metrics_dict[metric]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# save performance to output file\n",
    "date = datetime.now().date()\n",
    "i = 1\n",
    "while os.path.exists(TABLES_PATH + f'humor_results_params_{date}_{i}*.xlsx'):\n",
    "    i += 1\n",
    "\n",
    "result_param_df.to_excel(TABLES_PATH + f'humor_results_params_{date}_{i}.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}